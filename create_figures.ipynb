{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "002d10f5",
   "metadata": {},
   "source": [
    "# Creating and analysing the simulated datasets #\n",
    "Current code includes: \n",
    "Number of transcripts: each transcript has its own SNPs, each with their own error rate, and its own mobility status.\n",
    "\n",
    "Number of SNPs: the number of SNPs per transcript (currently fixed).\n",
    "\n",
    "Number of replicates: We create 2 homograft files (Nhom1 and Nhom2), and then no_reps heterograft files. \n",
    "\n",
    "# Method A #\n",
    "\n",
    "If a single SNP has a read depth >= min_read_thresh, then it is flagged as being mobile. If 1 or more SNPs are flagged as being mobile, then the transcript is flagged as being mobile.\n",
    "\n",
    "# Handling SNPs #\n",
    "\n",
    "Bayes Factors: sum them all (should we point out in the paper that this is a useful advantage of this method?)\n",
    "\n",
    "Method A: snp_thresh SNPs need to have reads mapping to the other ecotype, in order for a mobile classification to be given for the transcript\n",
    "\n",
    "Method B: snp_thres SNPs need to have reads mapping to the other ecotype, in order for a mobile classification to be given for the transcript\n",
    "\n",
    "# Handling replicates #\n",
    "\n",
    "Bayes Factors: Again, very simple. We sum across replicates\n",
    "\n",
    "Method A: A transcript needs to have been given a mobile assignment in rep_thresh replicates, in order for a final mobile assignment to be given.\n",
    "\n",
    "Method B: A transcript needs to have been given a mobile assignment in rep_thresh replicates, in order for a final mobiel assigment to be given."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e950f2bf",
   "metadata": {},
   "source": [
    "## Figure 4\n",
    "\n",
    " Filtering SNPs based on observed errors or determining mobile transcripts based on absolute read counts leads to poor classification accuracy. Using our Bayesian approach we assign a transcript as being graft-mobile if $\\log_{10} B_{21} \\ge 1$. Methods A and B are explained in the main text. Here we have three replicates per transcript. The Bayesian approach sums up the evidence over replicates whereas Methods A and B require that two out of three replicates show evidence for mobility. Both plots shows the accuracy, (TP + TN) / (TP + TN + FP + FN), of each method over 1000 simulated datasets for different read depths, $N$, for an error rate of $q=0.01$. The left plot shows how the accuracy varies for different values of $N$ for homograft read depths equal to $N$. The right plot shows how the accuracy varies for different values of $N$ for fixed homograft read depths of 1000. The convergence of Method A and B towards $\\approx$0.5 is a consequence of the balanced dataset with a 1:1 ratio of mobile:non-mobile transcripts, meaning that their performance is essentially little better than random. For an unbalanced dataset the accuracy could drop below 0.5 for Method A and B. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418246fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code to run the simulations for Figure 4\n",
    "import baymobil.baymobil as baymob\n",
    "import baymobil.simulations as sim\n",
    "import baymobil.plot_data as plot_data\n",
    "\n",
    "## Run the code in the folder for Figure 4\n",
    "%cd   \"/Users/tomkinsm/baymobil/Figure4/N/\"\n",
    "## Running and evaluating the simulations\n",
    "## Each simulation is designed to investigate the accuracy of the results as a function of some parameter. The values for this parameter must be defined in the \"parameters.cfg\" file\n",
    "parameter_func = \"N\"\n",
    "## Create the datasets. This function creates no_reps .csv files, storing them in the \"output\" folder. This will delete any previous files, so make sure you have moved anything that you still need.\n",
    "sim.create_simulated_data(parameter_func)\n",
    "## Now, we load in and combine all of the replicate data, according to the above rules\n",
    "df = plot_data.load_data()\n",
    "## Plot and compare the accuracy (TP + TN) / (TP + TN + FP + FN) of the three different methods on our simulated datasets\n",
    "plot_data.plot_data_all(df,parameter_func)\n",
    "\n",
    "%cd   \"/Users/tomkinsm/baymobil/Figure4/fixedNhom/\"\n",
    "parameter_func = \"N\"\n",
    "sim.create_simulated_data(parameter_func)\n",
    "df = plot_data.load_data()\n",
    "plot_data.plot_data_all(df, parameter_func)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68aa73ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code to run the simulations for Figure 5\n",
    "\n",
    "\n",
    "import baymobil.baymobil as baymob\n",
    "import baymobil.simulations as sim\n",
    "import baymobil.plot_data as plot_data\n",
    "import pandas as pd\n",
    "\n",
    "%cd   \"/Users/tomkinsm/baymobil/Figure5/\"\n",
    "sim.create_simulated_data(\"N\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f817bef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load in and process the data\n",
    "## As there are two parameters being varied here (N and q), we can't use the built in plotting function\n",
    "\n",
    "## Load in the data\n",
    "df = pd.read_csv(\"SNP_wise_values.csv\")\n",
    "## Group by transcript\n",
    "print(df)\n",
    "\n",
    "df_transcript = df.groupby([\"transcript\",\"N\",\"q\"]).sum().reset_index()\n",
    "print(df_transcript)\n",
    "\n",
    "## Calculate TP, TN, FP, FN\n",
    "df_transcript[\"TP\"] = 0\n",
    "df_transcript[\"TN\"] = 0\n",
    "df_transcript[\"FP\"] = 0\n",
    "df_transcript[\"FN\"] = 0\n",
    "\n",
    "df_transcript.loc[(df_transcript.mobile > 0) & (df_transcript.log10BF>=1),\"TP\"] = 1\n",
    "df_transcript.loc[(df_transcript.mobile == 0) & (df_transcript.log10BF<1),\"TN\"] = 1\n",
    "df_transcript.loc[(df_transcript.mobile == 0) & (df_transcript.log10BF>=1),\"FP\"] = 1\n",
    "df_transcript.loc[(df_transcript.mobile > 0) & (df_transcript.log10BF<1),\"FN\"] = 1\n",
    "\n",
    "print(df_transcript)\n",
    "\n",
    "final_results = df_transcript.groupby([\"N\",\"q\"]).sum()[[\"TP\",\"TN\",\"FP\",\"FN\"]].reset_index()\n",
    "\n",
    "print(final_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0058ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot the pie-chart plots\n",
    "## Create the figures separately and then add in the axes later\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_pie(x, ax):\n",
    "\n",
    "    tp = float(x[\"TP\"])\n",
    "    tn = float(x[\"TN\"]) \n",
    "    fp = float(x[\"FP\"]) \n",
    "    fn = float(x[\"FN\"]) \n",
    "\n",
    "    total = tp + tn + fp + fn\n",
    "    \n",
    "    tp = tp / total\n",
    "    tn = tn / total\n",
    "    fp = fp / total\n",
    "    fn = fn / total\n",
    "\n",
    "    n = float(x[\"N\"])\n",
    "    q = float(x[\"q\"])\n",
    "\n",
    "    ax.pie([tp,tn,fp,fn], center = ([n, q]), radius = 0.05, colors=['b','g','r','y'], normalize = True)\n",
    "\n",
    "matplotlib.rcParams.update({'font.size': 22})\n",
    "fig, ax = plt.subplots(1,1, figsize=(8, 8), dpi=300)\n",
    "fig.patch.set_facecolor('white')\n",
    "\n",
    "## Bayes factors\n",
    "df_transcript = df.groupby([\"transcript\",\"N\",\"q\"]).sum().reset_index()\n",
    "\n",
    "## Calculate TP, TN, FP, FN\n",
    "df_transcript[\"TP\"] = 0\n",
    "df_transcript[\"TN\"] = 0\n",
    "df_transcript[\"FP\"] = 0\n",
    "df_transcript[\"FN\"] = 0\n",
    "\n",
    "df_transcript.loc[(df_transcript.mobile > 0) & (df_transcript.log10BF>=1),\"TP\"] = 1\n",
    "df_transcript.loc[(df_transcript.mobile == 0) & (df_transcript.log10BF<1),\"TN\"] = 1\n",
    "df_transcript.loc[(df_transcript.mobile == 0) & (df_transcript.log10BF>=1),\"FP\"] = 1\n",
    "df_transcript.loc[(df_transcript.mobile > 0) & (df_transcript.log10BF<1),\"FN\"] = 1\n",
    "\n",
    "final_results = df_transcript.groupby([\"N\",\"q\"]).sum()[[\"TP\",\"TN\",\"FP\",\"FN\"]].reset_index()\n",
    "test = final_results.copy()\n",
    "test[\"N\"] = test[\"N\"] / test[\"N\"].max()\n",
    "test[\"q\"] = test[\"q\"] / test[\"q\"].max()\n",
    "test.apply(lambda x: plot_pie(x,ax), axis=1)\n",
    "x_ticks = np.arange(100,1000,100)\n",
    "y_ticks = np.arange(0.001,0.01,0.001)\n",
    "#plt.xticks(x_ticks)\n",
    "#plt.yticks(y_ticks)\n",
    "#plt.legend(['TP', 'TN','FP','FN'])\n",
    "ax.axis('tight')\n",
    "\n",
    "plt.savefig(\"bayes_pie.svg\",dpi=300)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "## Same plot for Method A\n",
    "fig, ax = plt.subplots(1,1, figsize=(8, 8), dpi=300)\n",
    "fig.patch.set_facecolor('white')\n",
    "\n",
    "snp_thresh = 2\n",
    "df_transcript = df.groupby([\"transcript\",\"N\",\"q\"]).sum().reset_index()\n",
    "## Calculate TP, TN, FP, FN\n",
    "df_transcript[\"TP\"] = 0\n",
    "df_transcript[\"TN\"] = 0\n",
    "df_transcript[\"FP\"] = 0\n",
    "df_transcript[\"FN\"] = 0\n",
    "\n",
    "df_transcript.loc[(df_transcript.mobile > 0) & (df_transcript.Method_A>=snp_thresh),\"TP\"] = 1\n",
    "df_transcript.loc[(df_transcript.mobile == 0) & (df_transcript.Method_A<snp_thresh),\"TN\"] = 1\n",
    "df_transcript.loc[(df_transcript.mobile == 0) & (df_transcript.Method_A>=snp_thresh),\"FP\"] = 1\n",
    "df_transcript.loc[(df_transcript.mobile > 0) & (df_transcript.Method_A<snp_thresh),\"FN\"] = 1\n",
    "\n",
    "final_results = df_transcript.groupby([\"N\",\"q\"]).sum()[[\"TP\",\"TN\",\"FP\",\"FN\"]].reset_index()\n",
    "test = final_results.copy()\n",
    "test[\"N\"] = test[\"N\"] / test[\"N\"].max()\n",
    "test[\"q\"] = test[\"q\"] / test[\"q\"].max()\n",
    "test.apply(lambda x: plot_pie(x,ax), axis=1)\n",
    "x_ticks = np.arange(100,1000,100)\n",
    "y_ticks = np.arange(0.001,0.01,0.001)\n",
    "#plt.xticks(x_ticks)\n",
    "#plt.yticks(y_ticks)\n",
    "#plt.legend(['TP', 'TN','FP','FN'])\n",
    "#ax.set_title(\"Method A\")\n",
    "ax.axis('tight')\n",
    "plt.savefig(\"methoda_pie.svg\",dpi=300)\n",
    "plt.show()\n",
    "\n",
    "## Same plot for Method B\n",
    "fig, ax = plt.subplots(1,1, figsize=(8, 8), dpi=300)\n",
    "fig.patch.set_facecolor('white')\n",
    "df_transcript = df.groupby([\"transcript\",\"N\",\"q\"]).sum().reset_index()\n",
    "\n",
    "## Calculate TP, TN, FP, FN\n",
    "df_transcript[\"TP\"] = 0\n",
    "df_transcript[\"TN\"] = 0\n",
    "df_transcript[\"FP\"] = 0\n",
    "df_transcript[\"FN\"] = 0\n",
    "\n",
    "df_transcript.loc[(df_transcript.mobile > 0) & (df_transcript.Method_B>=snp_thresh),\"TP\"] = 1\n",
    "df_transcript.loc[(df_transcript.mobile == 0) & (df_transcript.Method_B<snp_thresh),\"TN\"] = 1\n",
    "df_transcript.loc[(df_transcript.mobile == 0) & (df_transcript.Method_B>=snp_thresh),\"FP\"] = 1\n",
    "df_transcript.loc[(df_transcript.mobile > 0) & (df_transcript.Method_B<snp_thresh),\"FN\"] = 1\n",
    "\n",
    "final_results = df_transcript.groupby([\"N\",\"q\"]).sum()[[\"TP\",\"TN\",\"FP\",\"FN\"]].reset_index()\n",
    "\n",
    "test = final_results.copy()\n",
    "test[\"N\"] = test[\"N\"] / test[\"N\"].max()\n",
    "test[\"q\"] = test[\"q\"] / test[\"q\"].max()\n",
    "test.apply(lambda x: plot_pie(x,ax), axis=1)\n",
    "#plt.legend(['TP', 'TN','FP','FN'])\n",
    "##ax.set_title(\"Method B\")\n",
    "ax.axis('tight')\n",
    "plt.savefig(\"methodb_pie.svg\",dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73db9469",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run a test of the Thieme data, to see if the new code gives the same results\n",
    "\n",
    "import baymobil.baymobil as baymob\n",
    "import baymobil.simulations as sim\n",
    "import baymobil.plot_data as plot_data\n",
    "import os\n",
    "from os import listdir, mkdir, getcwd\n",
    "from os.path import isfile, join, isdir\n",
    "import pandas as pd\n",
    "\n",
    "# Navigate to the correct folder\n",
    "%cd   \"/Users/tomkinsm/mRNA_analysis/\"\n",
    "\n",
    "## Thieme homfiles dictionary\n",
    "def get_hom_data_thieme(filename):\n",
    "    if filename.lower().startswith('col'):\n",
    "        if \"root\" in filename:\n",
    "            hom1 = \"C-C-root-FN.csv\"\n",
    "            hom2 = \"P-P-root-FN.csv\"\n",
    "        else:\n",
    "            hom1 = \"C-C-shoot-FN.csv\"\n",
    "            hom2 = \"P-P-shoot-FN.csv\"\n",
    "    if filename.lower().startswith('ped'):\n",
    "        if \"root\" in filename:\n",
    "            hom2 = \"C-C-root-FN.csv\"\n",
    "            hom1 = \"P-P-root-FN.csv\"\n",
    "        else:\n",
    "            hom2 = \"C-C-shoot-FN.csv\"\n",
    "            hom1 = \"P-P-shoot-FN.csv\"\n",
    "    if filename.startswith('FN'):\n",
    "        ## FN files: col shoot, ped root\n",
    "        if \"root\" in filename:\n",
    "            hom2 = \"C-C-root-FN.csv\"\n",
    "            hom1 = \"P-P-root-FN.csv\"\n",
    "        else:\n",
    "            hom1 = \"C-C-shoot-FN.csv\"\n",
    "            hom2 = \"P-P-shoot-FN.csv\"\n",
    "    return hom1, hom2\n",
    "\n",
    "    ## Thieme / col-ped data\n",
    "def format_hom_data_col_ped(hompath, thisfile):\n",
    "    df = pd.read_csv(hompath + thisfile, delimiter=\"\\t\", low_memory = False)\n",
    "    df = df[[\"SNP\",\"depth\",\"colDepth\",\"lerDepth\"]]\n",
    "    ## Different handling based on file contents\n",
    "    if thisfile.lower().startswith(\"c\"):\n",
    "        df.rename(columns={\"colDepth\":\"eco1\",\"lerDepth\":\"n\", \"depth\":\"N\"}, inplace=True)\n",
    "    elif thisfile.lower().startswith(\"p\"):\n",
    "        df.rename(columns={\"lerDepth\":\"eco1\",\"colDepth\":\"n\", \"depth\":\"N\"}, inplace=True)\n",
    "    else: \n",
    "        print(\"Error!\")\n",
    "    cols = df.columns.drop(\"SNP\")\n",
    "    df[cols] = df[cols].apply(pd.to_numeric, errors='coerce')\n",
    "    df.dropna(inplace=True)\n",
    "    df = df[df[\"N\"]>0]\n",
    "    filename = thisfile.split(\".\")[0]\n",
    "    ## We need a new folder for the output files: check if this exists\n",
    "    clean_data_path = os.path.join(hompath, \"clean_data/\")\n",
    "    if os.path.exists(clean_data_path) == False:\n",
    "        os.makedirs(clean_data_path)\n",
    "    df.to_csv(clean_data_path + filename + \".csv\",index=None)\n",
    "\n",
    "def format_het_data_col_ped(hetpath, thisfile):\n",
    "    df = pd.read_csv(hetpath + thisfile, delimiter=\"\\t\", low_memory = False)\n",
    "    df = df[[\"SNP\",\"depth\",\"colDepth\",\"lerDepth\"]]\n",
    "    ## Different handling based on file contents\n",
    "    if thisfile.lower().startswith(\"col\"):\n",
    "        df.rename(columns={\"colDepth\":\"eco1\",\"lerDepth\":\"n\", \"depth\":\"N\"}, inplace=True)\n",
    "    elif thisfile.lower().startswith(\"ped\"):\n",
    "        df.rename(columns={\"lerDepth\":\"eco1\",\"colDepth\":\"n\", \"depth\":\"N\"}, inplace=True)\n",
    "    else: \n",
    "        if \"root\" in thisfile:\n",
    "            df.rename(columns={\"lerDepth\":\"eco1\",\"colDepth\":\"n\", \"depth\":\"N\"}, inplace=True)\n",
    "        else: df.rename(columns={\"lerDepth\":\"n\",\"colDepth\":\"eco1\", \"depth\":\"N\"}, inplace=True)\n",
    "    cols = df.columns.drop(\"SNP\")\n",
    "    df[cols] = df[cols].apply(pd.to_numeric, errors='coerce')\n",
    "    df.dropna(inplace=True)\n",
    "    df = df[df[\"N\"]>0]\n",
    "    filename = thisfile.split(\".\")[0]\n",
    "    ## We need a new folder for the output files: check if this exists\n",
    "    clean_data_path = os.path.join(hetpath, \"clean_data/\")\n",
    "    if os.path.exists(clean_data_path) == False:\n",
    "        os.makedirs(clean_data_path)\n",
    "    outpath = \"clean_data/\"\n",
    "    df.to_csv(clean_data_path + filename + \".csv\",index=None)\n",
    "\n",
    "def format_data(path, dataset):\n",
    "    print(dataset)\n",
    "    hetpath = path + \"/hetfiles/\"\n",
    "    hompath = path + \"/homfiles/\"\n",
    "    ## Load in the raw data files\n",
    "    hetfiles = [f for f in listdir(hetpath) if isfile(join(hetpath, f))]\n",
    "    homfiles = [f for f in listdir(hompath) if isfile(join(hompath, f))]\n",
    "    ## Next line will allow us to ignore .ds_store\n",
    "    hetfiles = [f for f in hetfiles if not f.startswith('.')]\n",
    "    homfiles = [f for f in homfiles if not f.startswith('.')]\n",
    "    ## Get the homograft experiments, rather than the replicates\n",
    "    homexps = [f.split('_')[0:3] for f in homfiles]\n",
    "    homexps = ['_'.join(f) for f in homexps]\n",
    "    homexps = set(homexps)\n",
    "    ## Choose the correct function\n",
    "    if dataset==\"thieme\":\n",
    "        hom_function = format_hom_data_col_ped\n",
    "        het_function = format_het_data_col_ped\n",
    "    #if dataset==\"col_ler\":\n",
    "    #    hom_function = format_hom_data_col_ler\n",
    "    #    het_function = format_het_data_col_ler\n",
    "    #if dataset==\"col_ler_meth\":\n",
    "    #    hom_function = format_hom_data_col_ler_meth\n",
    "    #    het_function = format_het_data_col_ler_meth\n",
    "    ## Run the formatting functions\n",
    "    print(\"Run hom function\")\n",
    "    [hom_function(hompath, f) for f in homexps]\n",
    "    print(\"Run het function\")\n",
    "    [het_function(hetpath, f) for f in hetfiles]\n",
    "\n",
    "def run_analysis(dataset, hetpath, hompath,filename):\n",
    "    if dataset == \"thieme\":\n",
    "        hom1, hom2 = get_hom_data_thieme(filename)\n",
    "    if dataset == \"col_ler\":\n",
    "        hom1, hom2 = get_hom_data_col_ler(filename)\n",
    "    if dataset == \"col_ler_meth\":\n",
    "        hom1, hom2 = get_hom_data_col_ler_meth(filename)\n",
    "    hom1 = hompath + hom1\n",
    "    hom2 = hompath + hom2\n",
    "    filename = hetpath + filename\n",
    "    print(hom1, hom2, filename)\n",
    "    baymob.run_bayes_analysis([hom1, hom2, filename])\n",
    "\n",
    "path = \"raw_data/\"\n",
    "path_thieme = path + \"thieme_test\"\n",
    "\n",
    "format_data(path_thieme, \"thieme\")\n",
    "## thieme\n",
    "hetpath = path_thieme + \"/hetfiles/clean_data/\"\n",
    "hompath = path_thieme + \"/homfiles/clean_data/\"\n",
    "hetfiles = [f for f in listdir(hetpath) if isfile(join(hetpath, f))]\n",
    "hetfiles = [f for f in hetfiles if not f.startswith('.')]\n",
    "## Also need to ignore any results files\n",
    "hetfiles = [f for f in hetfiles if not \"results\" in f]\n",
    "hetfiles = [f.replace(\"txt\", \"csv\") for f in hetfiles]\n",
    "\n",
    "for f in hetfiles:\n",
    "    print(f)\n",
    "    run_analysis(\"thieme\",hetpath, hompath,f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ad9e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add in the transcript info for each dataframe\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "snp_file = \"all_snps_annotation.txt\"\n",
    "thieme_path = \"raw_data/thieme_test/\"\n",
    "col_ler_path = \"raw_data/col_ler/\"\n",
    "col_ler_meth_path = \"raw_data/col_ler_meth/\"\n",
    "\n",
    "## Function to count the number of unique transcripts for each SNP\n",
    "def get_transcript_counts(row):\n",
    "    ## Get unique values in list\n",
    "    row_split = row.split(\",\")\n",
    "    ## Need to remove splice variants\n",
    "    row_no_splice = []\n",
    "    for item in row_split:\n",
    "        item = item.split(\"_\")[0]\n",
    "        row_no_splice.append(item)\n",
    "    row_no_splice = set(row_no_splice)\n",
    "    return len(row_no_splice)\n",
    "\n",
    "def create_snp_file(thispath):\n",
    "    df_snp = pd.read_csv(thispath + snp_file, delimiter = \"\\t\")\n",
    "    df_snp.dropna(subset =[\"transcripts\"], inplace=True)\n",
    "    ## Count the number of unique transcripts associated with each SNP\n",
    "    df_snp[\"Transcript count\"] = df_snp[\"transcripts\"].apply(lambda x: get_transcript_counts(x))\n",
    "    df_snp[\"transcripts\"] = df_snp[\"transcripts\"].apply(lambda x: x.split('_')[0])\n",
    "    df_snp = df_snp[df_snp[\"Transcript count\"]==1]\n",
    "    df_snp[[\"SNP\",\"transcripts\"]].to_csv(thispath + \"snp_ref.csv\", index=None)\n",
    "\n",
    "## Apply to all of the data\n",
    "create_snp_file(thieme_path)\n",
    "#create_snp_file(col_ler_path)\n",
    "#create_snp_file(col_ler_meth_path)\n",
    "\n",
    "# Make large dateset (col_ler / dn_ler)\n",
    "import os\n",
    "from os import listdir, mkdir, getcwd\n",
    "from os.path import isfile, join, isdir\n",
    "import pandas as pd\n",
    "\n",
    "col_ler_path = \"raw_data/col_ler/\"\n",
    "col_ler_meth_path = \"raw_data/col_ler_meth/\"\n",
    "thieme_path = \"raw_data/thieme_test/\"\n",
    "snp_file = \"raw_data/col_ler_meth/snp_ref.csv\"\n",
    "\n",
    "## Set the snp_ref for each dataset\n",
    "#col_ler_snp_ref = col_ler_path + snp_file\n",
    "#col_ler_meth_snp_ref = col_ler_meth_path + snp_file\n",
    "\n",
    "col_ler_results_path = col_ler_path + \"hetfiles/clean_data/\"\n",
    "thieme_results_path = thieme_path + \"hetfiles/clean_data/\"\n",
    "\n",
    "## Set the experiments for each dataset\n",
    "files = [f for f in listdir(thieme_results_path) if isfile(join(thieme_results_path, f))]\n",
    "results_files = [f for f in files if \"results\" in f]\n",
    "\n",
    "## Separate into replicate and experiment\n",
    "reps = [f for f in files if \"results\" not in f]\n",
    "\n",
    "reps = [f.split(\".\")[0] for f in reps]\n",
    "\n",
    "exps = [f.split(\"_\")[0:3] for f in results_files]\n",
    "\n",
    "exps = [\"_\".join(f) for f in exps]\n",
    "\n",
    "exps = set(exps)\n",
    "\n",
    "## Set the results files for each dataset\n",
    "\n",
    "snp_file = pd.read_csv(\"raw_data/thieme_test/snp_ref.csv\")\n",
    "\n",
    "thieme_results_path = \"raw_data/thieme_test/hetfiles/clean_data/\"\n",
    "\n",
    "files = [f for f in listdir(thieme_results_path) if isfile(join(thieme_results_path, f))]\n",
    "results_files = [f for f in files if \"results\" in f]\n",
    "reps = [f.split(\".\")[0] for f in files if \"results\" not in f]\n",
    "\n",
    "exps = [\"Col-FN-root\",\n",
    "         \"Col-FN-shoot\",\n",
    "         \"Ped-FN-root\",\n",
    "         \"Ped-FN-shoot\",\n",
    "         \"Col-N-root\",\n",
    "         \"Col-P-root\",\n",
    "         \"Ped-N-shoot\",\n",
    "         \"Ped-P-shoot\",\n",
    "         \"FN_stemUpper\",\"FN_flower\", \"FN_root\",\"FN_stemLower\",\"FN_rosette\"]\n",
    "\n",
    "print(reps)\n",
    "print(exps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811cc632",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compare the old and the new results\n",
    "\n",
    "results_file = files[0]\n",
    "old_path = \"raw_data/thieme/\"\n",
    "new_path = \"raw_data/thieme_test/\"\n",
    "file_path = \"hetfiles/clean_data/\"\n",
    "\n",
    "df_old = pd.read_csv(old_path + file_path + results_file)\n",
    "display(df_old)\n",
    "df_new = pd.read_csv(new_path + file_path + results_file)\n",
    "display(df_new)\n",
    "\n",
    "problem_snps = df_old[df_old[\"log10BF\"]==-2][\"SNP\"].to_list()\n",
    "\n",
    "print(df_old[df_old[\"log10BF\"]==-2])\n",
    "\n",
    "df_new[df_new[\"SNP\"].isin(problem_snps)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2f4e23db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:06<00:00,  1.99it/s]\n"
     ]
    }
   ],
   "source": [
    "## Create one big dataframe\n",
    "\n",
    "## Load in all of the results files and concatenate into one large dataframe\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def create_final_dataframe(exps, reps, results_files, snp_file):\n",
    "    ## Create the blank dataframe\n",
    "    df_list = []\n",
    "\n",
    "    for exp in tqdm(exps):\n",
    "        these_reps = [x for x in reps if x.startswith(exp)]\n",
    "        for rep in these_reps:\n",
    "            ## Load in the results file\n",
    "            bf_file = pd.read_csv(\"raw_data/thieme_test/hetfiles/clean_data/\" + rep + \"_results.csv\") \n",
    "            bf_file[\"exp\"] = exp\n",
    "            bf_file[\"rep\"] = rep\n",
    "            bf_file = pd.merge(bf_file, snp_file, on=\"SNP\")\n",
    "            df_list.append(bf_file)\n",
    "\n",
    "    df_final = pd.concat(df_list)\n",
    "    return df_final\n",
    "\n",
    "my_df = create_final_dataframe(exps, reps, results_files, snp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5375e3bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcripts</th>\n",
       "      <th>log10BF</th>\n",
       "      <th>exp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>105585</th>\n",
       "      <td>AT3G09260</td>\n",
       "      <td>598.064483</td>\n",
       "      <td>FN_stemLower</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105584</th>\n",
       "      <td>AT3G09260</td>\n",
       "      <td>307.264481</td>\n",
       "      <td>FN_rosette</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7323</th>\n",
       "      <td>AT1G07930</td>\n",
       "      <td>270.483658</td>\n",
       "      <td>FN_root</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161584</th>\n",
       "      <td>AT4G21960</td>\n",
       "      <td>252.180413</td>\n",
       "      <td>Ped-FN-shoot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26737</th>\n",
       "      <td>AT1G29930</td>\n",
       "      <td>236.569515</td>\n",
       "      <td>FN_root</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201422</th>\n",
       "      <td>AT5G24740</td>\n",
       "      <td>-1129.257957</td>\n",
       "      <td>Ped-N-shoot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201423</th>\n",
       "      <td>AT5G24740</td>\n",
       "      <td>-1172.573343</td>\n",
       "      <td>Ped-P-shoot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91124</th>\n",
       "      <td>AT2G42270</td>\n",
       "      <td>-1183.075777</td>\n",
       "      <td>Ped-N-shoot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91123</th>\n",
       "      <td>AT2G42270</td>\n",
       "      <td>-1200.121281</td>\n",
       "      <td>Ped-FN-shoot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91125</th>\n",
       "      <td>AT2G42270</td>\n",
       "      <td>-1216.000569</td>\n",
       "      <td>Ped-P-shoot</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>233727 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       transcripts      log10BF           exp\n",
       "105585   AT3G09260   598.064483  FN_stemLower\n",
       "105584   AT3G09260   307.264481    FN_rosette\n",
       "7323     AT1G07930   270.483658       FN_root\n",
       "161584   AT4G21960   252.180413  Ped-FN-shoot\n",
       "26737    AT1G29930   236.569515       FN_root\n",
       "...            ...          ...           ...\n",
       "201422   AT5G24740 -1129.257957   Ped-N-shoot\n",
       "201423   AT5G24740 -1172.573343   Ped-P-shoot\n",
       "91124    AT2G42270 -1183.075777   Ped-N-shoot\n",
       "91123    AT2G42270 -1200.121281  Ped-FN-shoot\n",
       "91125    AT2G42270 -1216.000569   Ped-P-shoot\n",
       "\n",
       "[233727 rows x 3 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_df.groupby([\"transcripts\",\"exp\"]).sum().reset_index()[[\"transcripts\",\"log10BF\",\"exp\"]].sort_values(\"log10BF\",ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f07bfa8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
